This is a demonstration  how to combine multiple classical graphics approaches with AI generated art to create a 3D web-based experience. 
The approaches used include text to image, image to video, text to 3D, and video to 3D using a technique called gaussian splatting.

To run this locally and experiment with modifying the code, clone this repository, and run the following commands from the root of the folder.\
    ```npm i```\
    ```npm run dev```\

Here are some links to the tools used for creating this:

[Dalle3](https://chat.openai.com/?model=gpt-4) (running within ChatGPT). Text to Image model (Used to generated stained glass and other textures)\
[Blockade Labs](https://www.blockadelabs.com/) A tool for creating 3D environment maps using a combination of prompting and sketching.\
[Magnific.AI](https://magnific.ai/) An image upscaler and enhancer\
[RunwayML](https://runwayml.com/) A text to video and image to video tool\
[Krea.AI](https://www.krea.ai/apps/image/realtime) A tool that allows the user to compose various inputs such as webcam capture, screen capture, and shapes, with a prompt and outputs images based on these in realtime\
[LumaWebGL Library](https://lumalabs.ai/luma-web-library) A library that renders scenes captured from the luma app using webgl.\ 
[Genie] A text to 3D model tool. (Used here for the candles and sculpture of the woman)\
[Blender](https://www.blender.org/) A free and open-source 3D content creation tool.\ 


[chapel model](https://sketchfab.com/3d-models/updated-chapel-model-7-12-19-275d95e2b4304d26b0010f915969ae6c) by adamleescott 